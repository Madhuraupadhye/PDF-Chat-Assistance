ğŸ“„ Creating a Large Language Model â€“ PDF Chat Assistant

A conversational AI system that allows users to chat with their PDF documents using a Large Language Model (LLM).
Built with LangChain, FAISS, and GPT-Neo 2.7B on Streamlit.

ğŸ§  Overview

The PDF Chat Assistant is an intelligent application that lets you upload PDFs and interact with them conversationally.
It uses OCR for scanned files, creates semantic embeddings from text, and retrieves relevant content using FAISS.
Responses are generated contextually using the GPT-Neo 2.7B model hosted on Hugging Face.

In simple terms â€” itâ€™s like a mini ChatGPT that answers only from your uploaded PDF.

ğŸš€ Features

âœ… Upload multiple PDFs
âœ… OCR support for scanned/image-based PDFs
âœ… Semantic search using FAISS
âœ… GPT-Neo 2.7B for contextual responses
âœ… Conversation memory for multi-turn chat
âœ… Modern and interactive Streamlit UI
âœ… Error handling for invalid inputs or empty PDFs

âš™ï¸ Tech Stack
Component	Technology
Language	Python
Frontend	Streamlit
LLM	GPT-Neo 2.7B (via Hugging Face Hub)
Embeddings	Sentence Transformers (all-MiniLM-L6-v2)
Vector Store	FAISS
Text Extraction	PyPDF2, pytesseract, pdf2image
Framework	LangChain
Environment Management	python-dotenv
ğŸ§© Architecture
PDF Upload â†’ Text Extraction (PyPDF2 / OCR)
             â†“
      Text Chunking (LangChain)
             â†“
   Embedding Generation (MiniLM)
             â†“
      Vector Storage (FAISS)
             â†“
  Query + Retrieval (RAG pipeline)
             â†“
   Response Generation (GPT-Neo)
             â†“
        Streamlit Chat UI

ğŸ› ï¸ Installation and Setup

Clone the repository

git clone https://github.com/<your-username>/PDF-Chat-Assistant.git
cd PDF-Chat-Assistant


Create and activate a virtual environment

python -m venv venv
source venv/bin/activate     # For Mac/Linux
venv\Scripts\activate        # For Windows


*Install dependencies

pip install -r requirements.txt


*Add your Hugging Face token

* Create a .env file in the root folder:

HUGGINGFACEHUB_API_TOKEN=your_huggingface_token_here

* Run the app

streamlit run mainapp.py

ğŸ“š How It Works

Upload one or more PDF files. --> The system extracts text using PyPDF2. --> If text is not readable, it uses pytesseract OCR. --> The text is split into smaller chunks (size: 3000, overlap: 400).
--> Each chunk is converted into embeddings and stored using FAISS.--> When you ask a question, the model: 1. Retrieves relevant chunks from FAISS.
                                                                                                           2. Sends context to GPT-Neo 2.7B for response generation.
--> The chatbot displays the contextual answer interactively.

ğŸ§® Example Use Case
Query	Response
â€œWhat is the conclusion of this report?â€	Returns conclusion text from the PDF
â€œWho are the authors?â€	Extracts author names from the first page
â€œExplain the methodology used.â€	Generates a concise summary from relevant sections

ğŸ† Publication
Published in IJCRT (International Journal of Creative Research Thoughts): Check it out: https://www.ijcrt.org/papers/IJCRT2505614.pdf
Title: Creating a Large Language Model â€“ PDF Chat Assistant
Authors: Madhura Upadhye, Aditya Sarate, Utkarsha Chougule, Sourabh Gavandi, Mr Milind S. Vadagave
Guide: Mr. M. S. Vadagave
